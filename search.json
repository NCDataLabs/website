[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nic Crane",
    "section": "",
    "text": "Hi, I’m Nic! I’m a data scientist, software engineer, and R consultant. I am part of the team who maintain the Apache Arrow project and co-author of Scaling Up with R and Arrow.\nI’ve been using R for 15 years, and am an active member of various communities and sub-communities.\nDuring my career, I’ve worked across multiple industries, including pharma, public health, academia, and startups, and have worked on a real mix of projects, encompassing everything from teaching new programmers how to work with R, to maintaining popular open source packages, to delving into the complexities of deploying R code in production environments where architecture and scalability matter.\nSome highlights:"
  },
  {
    "objectID": "about.html#nc-data-labs",
    "href": "about.html#nc-data-labs",
    "title": "Nic Crane",
    "section": "NC Data Labs",
    "text": "NC Data Labs\nThe name NC Data Labs came about because I wanted to emphasise the interplay between using known processes and past experience with deploying creative solutions to the technical problems I’m asked to help solve.\nEvery project and client is unique and there is never a one-size-fits-all approach that applies universally. Many consulting projects with a broader scope involve the iterative process of developing a hypothesis, testing it out, examining the results, and then deciding what’s next based on drawn conclusions, and so I wanted the name of the company to reflect that.\nI also found working at Ursa Labs one of the most technically challenging and exciting parts of my career and also wanted to call out to that.\nGet in touch →"
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Services",
    "section": "",
    "text": "Here are some examples of the kinds of projects I work on:\n\n\nDesigning and Building Internal Data Tools\n\n\n\n\n\nMany of the previous projects I’ve worked on have focused on getting value out of existing data and building tools to help people make decisions or automate workflows.\nThis could be:\n\na Shiny app\nan automated report or dashboard\nan R package\nan API for connecting data science outputs with other systems\nsomething else entirely\n\nI’ve worked on projects which have been rolled out to the NHS, large scale open source projects, and internal tools for automating slow and manual processes.\nMy approach focuses on building something that delivers clear business value aligned with your goals, and is usable and supportable, not just technically functional.\n\n\n\nStrengthening R Code for Production Use\n\n\n\n\n\nData science is distinct from software engineering in the approach to creating data products and tools which best support the needs of an organisation; the development cycle is often much faster and tends to be supported by domain experts whose skills skew more towards the interpretation and use of the data, or complex modelling.\nThis can mean tools which are effective to get a process off the ground need additional work to ensure that they remain efficient, reliable, and maintainable as they scale to more business-critical usage or more users.\nSometimes this is as simple as ensuring code is well-tested and contains all the checks and oversight to ensure confidence in the output it provides.\nOther times, this might require more extensive refactoring, including things like:\n\nsimplifying overcomplicated code and implementing best practices to reduce time and money spent on maintenance and lowering the barrier to entry for those working with it\nsetting up CICD pipelines to automating the testing and building of code and reduce or remove manual effort\nimplementing software engineering practices like logging, versioned release strategies, and dependency management to ensure reliability and integration with other tools\n\nI can help transform your existing code into robust production-quality tools, taking a practical approach which accounts for where you are now, where you want to get to, but most importantly how to get there in a way which will be sustainable in the long-term without overcomplicating things.\n\n\n\nPlanning and Delivering Migrations to R from SAS, Stata, or Excel\n\n\n\n\n\nWorking with open source tooling like R has huge benefits like reducing or removing dependency on expensive proprietary solutions, allowing greater flexibility in what you can do with your data, enabling reproducible workflows, and being part of a huge community of practitioners that you can learn from and benefit being part of.\nMigrating from existing tools can present some challenges though - working out how to translate functionality from an existing legacy tool to R and reaping the benefits while minimising disruption.\nSome of the ways I can help with this transition include:\n\nupskilling new R users in R basics, focusing on the skills they need to do their job\nanalysing legacy codebases and helping you plan the transition to R\ncreating R packages, Shiny apps, and other data products to replace previous workflows\nproviding training and mentoring on R best practices\ndesigning and implementing processes for managing internal codebases in a sustainable manner\nand more!\n\nEvery code migration is different, but I can help you take this step, focusing both on the technical side of things as well as the people aspects - which are just as important but easily overlooked!\n\n\n\nGeneral R Consulting and Team Support\nI’m also available for more general R and data science consulting and team support. Whatever it is that you need, feel free to get in touch, and we can see if I can help you, or refer you on to other wonderful folks I know in the wider R and data science ecosystem.\n\nTake a look here to see some of my previous projects, or visit my contact page to get in touch."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NC Data Labs",
    "section": "",
    "text": "What I Do\nHi, I’m Nic! I’m a data scientist, software engineer, and R developer. I am part of the team who maintain the arrow R package and co-author of Scaling Up with R and Arrow.\nI help organisations get more value from their R code, whether that means designing and building internal tools from scratch, strengthening fragile workflows, or planning a move away from legacy systems like SAS or Excel.\nRead more about services →\n\n\n\nA Bit About Me\nI’ve spent the last decade working across data science, software engineering, and open source; contributing to the Apache Arrow R project, building tooling used in the NHS, and teaching at international R conferences.\nSee selected project work →\nMore about me →\n\n\n\nGet in Touch\nWhether you need a code audit, help designing a data tool, or want to talk through an upcoming migration, I’m happy to have a conversation.\nContact →"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Get in Touch",
    "section": "",
    "text": "If you’d like to talk about a project, need help with a code audit or tool development, or just want to sense-check an idea, feel free to reach out.\n\n  \n    Name\n    \n  \n  \n    Email\n    \n  \n  \n    Message\n    \n  \n  \n    Send"
  },
  {
    "objectID": "selected_work.html",
    "href": "selected_work.html",
    "title": "Selected Work",
    "section": "",
    "text": "Examples of how I’ve applied R, Shiny, and software engineering across internal tools, public-sector systems, and open-source projects.\n\n\nOpen Source Contribution – Apache Arrow Project\nType: Open Source Development & Authoring\n\n\n\n\n\nI’ve been an active contributor to the Apache Arrow R package, where I’ve worked on implementing features, acting as package maintainer, and extending dplyr support to Arrow. This work has influenced how I approach performance, reproducibility, and maintainability - both in open source and in client projects.\nDuring this time I also taught and wrote about Arrow:\n\nBig Data in R with Arrow – workshop taught at Posit Conf (2023, 2024)\n\nScaling up with R and Arrow – co-authored book on large-scale data workflows in R, due out in 2025\n\n\n\n\nSupporting Genomic Medicine in the NHS with R and Shiny\nType: Client Project (Team Build)\n\n\n\n\n\nI contributed to a modular Shiny application used in the national rollout of genomic testing within the NHS, building on the 100,000 Genomes Project. It let operational staff, lab teams, and programme managers track genomic samples and view key metrics without needing to interact directly with backend systems.\nWhat I worked on:\n- UI and server logic across multiple Shiny sub-apps\n- Integration with APIs and SQL databases\n- Interactive visualisations using plotly, visNetwork, sunburstR and other htmlwidget packages - Profiling, refactoring, and scaling reactive code for performance\nThe app was built to support a national rollout and continues to be used across NHS teams.\nWatch the talk →\n\n\n\nRefactoring a Legacy Shiny App for Maintainability\nType: Internal Tooling (Team Lead)\n\n\n\n\n\nWhile at Novartis, I led a small team responsible for maintaining an internal Shiny app used to manage R package installation requests. The codebase had become increasingly fragile and hard to maintain, with tightly coupled logic and growing technical debt.\nWhat I worked on:\n- Refactored reactive logic to simplify control flow and heavily modularised code to support maintainability\n- Introduced testing strategies and CI workflows\n- Improved onboarding for new developers by making the codebase easier to understand\nThe changes made the application significantly more maintainable and safer to extend as the team evolved.\n\n\n\nScoping a Large-Scale SAS to R Migration\nType: Consulting – Code Audit & Planning\n\n\n\n\n\nAs part of a consultancy engagement, I audited a large SAS codebase to help a client understand the feasibility and risk profile of migrating to R. The codebase was business-critical and had grown organically over many years.\nWhat I worked on:\n- Static code analysis of SAS codebase to estimate size and complexity (e.g. SQL-style vs. custom PROC logic)\n- Identified areas where statistical methods or parameter defaults might behave differently in R\n- Delivered a scoped migration plan to help inform budget, risk, and staffing decisions\nThe audit helped the client avoid assumptions about direct conversion and provided a structured way to evaluate next steps.\n\nGet in touch →"
  }
]