[
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Services",
    "section": "",
    "text": "Here are some examples of the kinds of projects I work on:\n\n\nDesigning and Building Internal Data Tools\n\n\n\n\n\nMany of the previous projects I’ve worked on have focused on getting value out of existing data and building tools to help people make decisions or automate workflows.\nThis could be:\n\na Shiny app\nan automated report or dashboard\nan R package\nan API for connecting data science outputs with other systems\nsomething else entirely\n\nI’ve worked on projects which have been rolled out to the NHS, large scale open source projects, and internal tools for automating slow and manual processes.\nMy approach focuses on building something that delivers clear business value aligned with your goals, and is usable and supportable, not just technically functional.\n\n\n\nStrengthening R Code for Production Use\n\n\n\n\n\nData science is distinct from software engineering in the approach to creating data products and tools which best support the needs of an organisation; the development cycle is often much faster and tends to be supported by domain experts whose skills skew more towards the interpretation and use of the data, or complex modelling.\nThis can mean tools which are effective to get a process off the ground need additional work to ensure that they remain efficient, reliable, and maintainable as they scale to more business-critical usage or more users.\nSometimes this is as simple as ensuring code is well-tested and contains all the checks and oversight to ensure confidence in the output it provides.\nOther times, this might require more extensive refactoring, including things like:\n\nsimplifying overcomplicated code and implementing best practices to reduce time and money spent on maintenance and lowering the barrier to entry for those working with it\nsetting up CICD pipelines to automating the testing and building of code and reduce or remove manual effort\nimplementing software engineering practices like logging, versioned release strategies, and dependency management to ensure reliability and integration with other tools\n\nI can help transform your existing code into robust production-quality tools, taking a practical approach which accounts for where you are now, where you want to get to, but most importantly how to get there in a way which will be sustainable in the long-term without overcomplicating things.\n\n\n\nPlanning and Delivering Migrations to R from SAS, Stata, or Excel\n\n\n\n\n\nWorking with open source tooling like R has huge benefits like reducing or removing dependency on expensive proprietary solutions, allowing greater flexibility in what you can do with your data, enabling reproducible workflows, and being part of a huge community of practitioners that you can learn from and benefit being part of.\nMigrating from existing tools can present some challenges though - working out how to translate functionality from an existing legacy tool to R and reaping the benefits while minimising disruption.\nSome of the ways I can help with this transition include:\n\nupskilling new R users in R basics, focusing on the skills they need to do their job\nanalysing legacy codebases and helping you plan the transition to R\ncreating R packages, Shiny apps, and other data products to replace previous workflows\nproviding training and mentoring on R best practices\ndesigning and implementing processes for managing internal codebases in a sustainable manner\nand more!\n\nEvery code migration is different, but I can help you take this step, focusing both on the technical side of things as well as the people aspects - which are just as important but easily overlooked!\n\n\n\nGeneral R Consulting and Team Support\nI’m also available for more general R and data science consulting and team support. Whatever it is that you need, feel free to get in touch, and we can see if I can help you, or refer you on to other wonderful folks I know in the wider R and data science ecosystem.\n\nTake a look here to see some of my previous projects, or visit my contact page to get in touch."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NC Data Labs",
    "section": "",
    "text": "What I Do\nHi, I’m Nic! I’m a data scientist, software engineer, and R developer. I am part of the team who maintain the arrow R package and co-author of Scaling Up with R and Arrow.\nI help organisations get more value from their R code, whether that means designing and building internal tools from scratch, strengthening fragile workflows, or planning a move away from legacy systems like SAS or Excel.\nRead more about services →\n\n\n\nA Bit About Me\nI’ve spent the last decade working across data science, software engineering, and open source; contributing to the Apache Arrow R project, building tooling used in the NHS, and teaching at international R conferences.\nSee selected project work →\nMore about me →\n\n\n\nGet in Touch\nWhether you need a code audit, help designing a data tool, or want to talk through an upcoming migration, I’m happy to have a conversation.\nContact →"
  },
  {
    "objectID": "blog/posts/rmigration.html",
    "href": "blog/posts/rmigration.html",
    "title": "Migrating to R - Challenges and Solutions",
    "section": "",
    "text": "Migrating to R isn’t just a technical rewrite; it’s a cultural and operational shift. I’ve been involved in multiple projects involving migrating to R from a previous solution like SAS or Excel, sometimes scoping out the roadmap for a complete transition, and other times doing the work itself on subcomponents of a wider project within a larger organisation. Every project is different, but common patterns (and pitfalls) emerge.\nIt’s easy to assume that it’s just about writing code, but in reality there’s more to consider here - it’s about culture, politics, and momentum as well."
  },
  {
    "objectID": "blog/posts/rmigration.html#why-migrations-to-r-are-harder-than-they-look",
    "href": "blog/posts/rmigration.html#why-migrations-to-r-are-harder-than-they-look",
    "title": "Migrating to R - Challenges and Solutions",
    "section": "Why Migrations to R Are Harder Than They Look",
    "text": "Why Migrations to R Are Harder Than They Look\nThere are lots of reasons people choose to migrate to R from other solutions, and these tend to be a mix of wanting to move away from expensive proprietary licenses, the skillsets of new data analysts and scientists skewing more towards modern technologies like R and Python, and wanting to implement more robust practices for code being used to support wider business processes. Factor in the ability to access cutting-edge statistical methods, interoperability with the wider modern tech stack, and scope for flexibility and customisation, and it’s hard to see why not to move.\n\nComplex existing codebases\nOne of the common themes I’ve seen in migrations is large codebases which feel vast and difficult to tackle. Many organisations depend on undocumented production code, with little understanding of its complexity or volume.\n\n\nSkillset gaps\nSkillsets are often mismatched with few people confident in both SAS and R. Much of the time, people are supporting legacy code which they’ve inherited from other programmers who have since moved on to other teams or left the organisation entirely and things may work but people don’t know how or why.\n\n\nMissing practices (version control, testing)\nIt’s also common to see codebases which just work but don’t follow modern practices such as using version control to track code changes and manage multiple people working on the same code, no tests or continuous integration and so the intended behaviour isn’t always well understood, is prone to bugs, and it’s impossible to confidently make changes without breaking something.\n\n\nOrganisational buy-in\nAnother challenge can be organisational commitment and buy-in. When there isn’t agreement around the change is necessary, it can be seen as time-consuming and difficult. If these transformations are not adequately supported and staff are expected to change tools without training or time to upskill, resistance is inevitable. Learning to program in R can be a rewarding experience, but without the proper setup can just be an annoyance.\nIt’s not as simple as taking code line by line and rewriting from one language to the other. Take the example of moving from SAS to R, the different frameworks have entirely different approaches, and a direct translation tends to leave people with inefficient code. Modern approaches like LLMs can help generate the equivalent code when switching over, but often a broader view is needed to do a good job of a transformation."
  },
  {
    "objectID": "blog/posts/rmigration.html#how-to-approach-migration-effectively",
    "href": "blog/posts/rmigration.html#how-to-approach-migration-effectively",
    "title": "Migrating to R - Challenges and Solutions",
    "section": "How to Approach Migration Effectively",
    "text": "How to Approach Migration Effectively\nWith a systematic approach which focuses on the right areas, it’s possible to do a successful migration in which the benefits are realised easily.\n\nAudit codebases: size, complexity, risk\nOne project I worked on was scoping out a migration for a large organisation which depended mostly on SAS, but had some R users, and had made the decision to transition away from SAS with a clear deadline for completion. I conducted an in-depth audit on their current setup and advised on the path forward.\nI started off by identifying the different stakeholders in the organisation. This is important as they tend to have divergent needs - people who are writing the code, people who are making decisions based on the results of their analyses, and people who are deploying the code in production environments.\nAuditing what you have is critical. As part of this project, I analysed the entire SAS codebase, scanning the code to work out how much there was and what the different SAS proc calls were. Auditing the code gives a clear view of the scope and complexity.\nWhen you have SAS code which uses, for example, mostly proc SQL, this is simpler to convert to the equivalent SQL in R. There are specific challenges which become apparent when you’re dealing with statistical analyses; there are some differences between R and SAS in terms of default parameter values and implementations of methodologies, but this can be tackled by identifying the key procs, and robustly testing outputs to compare them. There are also excellent projects like CAMIS which provide guidance on the differences between the different languages.\nThese audits must go beyond simple code analyses though; it’s also important to prioritise different areas of the codebase and distinguish between what is used in production versus what is more ad-hoc code.\n\n\nIdentify quick-win proof-of-concept projects\nAt this point, it’s often helpful to identify good candidates for proof-of-concept projects which allow clear demonstration of value and evidence of tangible benefits to the business. The best place to look is usually found by interviewing stakeholders and getting an idea of what really is difficult in their day-to-day work, or something which is particularly inefficient. This is where things like Shiny apps are excellent for transforming a time-consuming and manual process into something more engaging and easy to use.\nIn one project, users relied on a painful command-line workflow involving multiple configuration files and obscure commands. We built a Shiny app to wrap around this process with intuitive dropdowns, clear instructions, and a clean interface - massively improving usability and reducing error.\nQuick wins demonstrate value and generate the interest and engagement needed for a migration."
  },
  {
    "objectID": "blog/posts/rmigration.html#culture-matters-more-than-tools",
    "href": "blog/posts/rmigration.html#culture-matters-more-than-tools",
    "title": "Migrating to R - Challenges and Solutions",
    "section": "Culture Matters More Than Tools",
    "text": "Culture Matters More Than Tools\nUsers are where it’s important to focus most of your efforts.\n\nSupporting users through change\nUsers who are comfortable and confident in their existing tooling can be resistant to having that taken away from them. Change can be seen as disruptive and lead to pushback. Giving people quick wins in R is important so they can quickly build confidence.\n\n\nTraining, mentoring, lightweight standards\nA great way to get started is via training courses but there’s more that can be done here. Provide them with training on using R, and further support from experts - whether internal or external. Ensure that code is reviewed, but give clear guidance on how to conduct a good code review - make sure that it empowers and enables people rather than leaving them feeling criticised and incompetent.\nSupport can take different forms: one-on-one mentoring, team office hours, or embedding R champions within teams.\n\n\nBuilding community and momentum\nEither way, change happens when people understand the benefits and feel supported. And sometimes you just have to make it fun - for example, by running hackathons where people can blend new skills with creativity, or bringing in external speakers can also generate enthusiasm. Ultimately, you need some sense of community so people feel like they’re not just working away in isolation."
  },
  {
    "objectID": "blog/posts/rmigration.html#how-i-help",
    "href": "blog/posts/rmigration.html#how-i-help",
    "title": "Migrating to R - Challenges and Solutions",
    "section": "How I Help",
    "text": "How I Help\nI really enjoy these kinds of projects as it’s rewarding to see folks enthusiastically engage with tech which is new to them.\n\nAdvisory, proof-of-concept, guidance\nOn these kinds of projects, I tend to work in the role of advisor and implementer. What works well is to implement some kind of proof-of-concept project to both demonstrate value and then use as a prototype for what good looks like, and then provide guidance and support to teams updating their own codebases. I don’t tend to offer full codebase rewrites - the point is to enable teams to be self-reliant and empower them to be able to move independently in the longer term. That said, sometimes it makes sense for me to rewrite the more complex areas or provide additional support in terms of code reviews and introduction of best practices.\n\n\nTraining options\nI also offer training courses and support - whether that’s introductory R, more complex topics like working with version control, writing R packages, or a more bespoke course tailored to your needs.\nThe key thing here is providing structure and clarity, planning out the various stages, helping you identify what actions and infrastructure you’ll need in each part, and working out a path to make this work for you and your organisation.\n\n\nGet in Touch\nIf you’re planning a migration or are stuck partway through one and need experienced support, I can help you move it forward. Get in touch."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Prompt Design for LLMs in Shiny Apps - Exploring {querychat} in R\n\n\n\n\n\nAnalysing the use of best practices for system prompting in the {querychat} package\n\n\n\n\n\nMay 1, 2025\n\n\nNic Crane\n\n\n\n\n\n\n\n\n\n\n\n\nMigrating to R - Challenges and Solutions\n\n\n\n\n\nTechnical migrations projects and their challenges.\n\n\n\n\n\nApr 28, 2025\n\n\nNic Crane\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nic Crane",
    "section": "",
    "text": "Hi, I’m Nic! I’m a data scientist, software engineer, and R consultant. I am part of the team who maintain the Apache Arrow project and co-author of Scaling Up with R and Arrow.\nI’ve been using R for 15 years, and am an active member of various communities and sub-communities.\nDuring my career, I’ve worked across multiple industries, including pharma, public health, academia, and startups, and have worked on a real mix of projects, encompassing everything from teaching new programmers how to work with R, to maintaining popular open source packages, to delving into the complexities of deploying R code in production environments where architecture and scalability matter.\nSome highlights:"
  },
  {
    "objectID": "about.html#nc-data-labs",
    "href": "about.html#nc-data-labs",
    "title": "Nic Crane",
    "section": "NC Data Labs",
    "text": "NC Data Labs\nThe name NC Data Labs came about because I wanted to emphasise the interplay between using known processes and past experience with deploying creative solutions to the technical problems I’m asked to help solve.\nEvery project and client is unique and there is never a one-size-fits-all approach that applies universally. Many consulting projects with a broader scope involve the iterative process of developing a hypothesis, testing it out, examining the results, and then deciding what’s next based on drawn conclusions, and so I wanted the name of the company to reflect that.\nI also found working at Ursa Labs one of the most technically challenging and exciting parts of my career and also wanted to call out to that.\nGet in touch →"
  },
  {
    "objectID": "blog/posts/querychat.html",
    "href": "blog/posts/querychat.html",
    "title": "Prompt Design for LLMs in Shiny Apps - Exploring {querychat} in R",
    "section": "",
    "text": "LLMs can handle straightforward conversations fluently but the unpredictable nature of responses means that more careful thought is needed when when embedding LLMs in Shiny apps.\nA key area to focus on is effective prompt engineering, to enable more reliable responses and really scaffold how the model “thinks”.\nOne emerging use case in data science is allowing users to perform natural language queries against datasets without needing to formulate their questions as code or SQL queries. This lowers the barrier for entry for non-technical users and allows more flexible app workflow.\nIn preparation for my EARL 2025 workshop on integrating LLMs into Shiny apps, I explored the querychat package, written by Joe Cheng, to understand effective system prompt design for natural language data querying. It makes a great case study for effective prompt engineering for querying data.\nYou can see a live demo of the package in action here.\nIn this blog post, I’m going to take a look at some of the internals of the default system prompt used by querychat to show you the elements of its design you can use in other projects for working with LLMs to query data."
  },
  {
    "objectID": "blog/posts/querychat.html#the-system-prompt",
    "href": "blog/posts/querychat.html#the-system-prompt",
    "title": "Prompt Design for LLMs in Shiny Apps - Exploring {querychat} in R",
    "section": "The System Prompt",
    "text": "The System Prompt\nWhen designing software containing LLM components, it’s crucial to have a good system prompt. This is the prompt which governs the overall behaviour of the LLM and is set up before the user has the chance to interact with it.\nIn the book AI Engineering, Chip Huyen divides LLM prompts into 3 components:\n\nThe task description\nExamples of how to do the task\nA specific task request\n\nChip goes on to explain that items 1 and 2 should be included in the system prompt and then item 3 is the part that the user adds.\nThere are specific techniques which can be used to create an effective system prompt that reduce the chance of errors, inaccuracies, and other undesirable results.\nI’m going to take the querychat system prompt one section at a time and discuss the techniques being used. You can see the full prompt here.\n\nDefine a Role\nA useful starting point is asking the model to assume a specific role, which will guide the perspective it assumes when answering. This helps limit the scope of potential answers and makes results more predictable and consistent.\nThe querychat system prompt starts off with a straightforward description of the role of the LLM and a brief overview of the kinds of tasks it will be fulfilling.\n\nYou are a chatbot that is displayed in the sidebar of a data dashboard. You will be asked to perform various tasks on the data, such as filtering, sorting, and answering questions.\n\n\n\nAllow it to be unsure\nA common concern when working with LLMs is the risk of false information. By explicitly prompting the LLM on how to behave when it’s unsure, you reduce the chance of encountering confidently stated low-certainty responses.\nThe querychat system prompt tells the LLM to ask users additional questions to account for uncertainty instead of just trying to generate a response based on ambiguous inputs.\n\nIt’s important that you get clear, unambiguous instructions from the user, so if the user’s request is unclear in any way, you should ask for clarification. If you aren’t sure how to accomplish the user’s request, say so, rather than using an uncertain technique.\n\nAsking for clarification can be a useful guardrail, but it’s a trade-off between accuracy and frustrating users or slowing down simple tasks.\n\n\nGive it cues about output\nWithout any cues as to the format of the output, many LLMs tend to err on the side of being wordy and adding extra conversational sentences. Providing cues as to the desired output format helps to both cut down this information, as well as reducing costs and speeding up responses by just returning what is needed.\nThe querychat system prompt specifically mentions where the output will be displayed and explicitly mentions what not to include in the output.\n\nThe user interface in which this conversation is being shown is a narrow sidebar of a dashboard, so keep your answers concise and don’t include unnecessary patter, nor additional prompts or offers for further assistance.\n\n\n\nAdd context about the data\nIt’s not just what not to include which is important though. Even if you’re working with an app which doesn’t have the entire dataset sent to an LLM for security or cost reasons, you can still provide additional context to help guide the LLM.\nThe querychat system prompt for a particular session is set up using querychat_init(), into which is also passed the data frame to be queried. High-level summary data is extracted from the dataset, and this summary forms part of the system prompt.\nIn this case, I initialised querychat with the mtcars dataset, and so the system prompt contains a summary of the columns, including their names, data types, and value ranges.\n\nYou have at your disposal a DuckDB database containing this schema:\n\nTable: mtcars\nColumns:\n- mpg (FLOAT)\n  Range: 10.4 to 33.9\n- cyl (FLOAT)\n  Range: 4 to 8\n- disp (FLOAT)\n  Range: 71.1 to 472\n- hp (FLOAT)\n  Range: 52 to 335\n- drat (FLOAT)\n  Range: 2.76 to 4.93\n- wt (FLOAT)\n  Range: 1.513 to 5.424\n- qsec (FLOAT)\n  Range: 14.5 to 22.9\n- vs (FLOAT)\n  Range: 0 to 1\n- am (FLOAT)\n  Range: 0 to 1\n- gear (FLOAT)\n  Range: 3 to 5\n- carb (FLOAT)\n  Range: 1 to 8\n\n\nDefensive prompt engineering\nA potential risk in exposing LLMs to outside use is the potential use of prompt attacks to expose data which isn’t intended for public consumption or even just using the LLM for unauthorised purposes.\nDefending against these risks often requires checks at multiple levels depending on level of risk, but a simple approach for low-risk projects involves explicitly stating what information can and cannot be used.\nThe querychat system prompt explicitly refers to the data which can be used in returning a response.\n\nFor security reasons, you may only query this specific table.\n\nNote however, that this isn’t full solution to potential abuse, and in production systems it’s crucial to validate user input in more depth.\n\n\nDescribing specific tasks part 1\nWhen embedding conversational LLMs into an app, you may want the LLM to be responsible for different tasks. The Open AI guide to prompt engineering discusses a strategy for returning the most relevant information by breaking this down into 2 steps:\n\nIntent classification - working out what it is that the user wants to achieve\nProviding additional instructions based on the user’s intent\n\nIn a simple app, this multi-stage implementation isn’t necessary, and providing a list of possible tasks with relevant instructions can suffice.\nThe querychat system prompt does this, describing different tasks, including the usage of tools to return the relevant information. The instructions contain lots of detail, and we can infer that these clarifications were likely added to the prompt iteratively by experimenting with shorter instructions and seeing what needed to be added to get consistent results returned. This is typical of iterative prompt design and is a key part of prompt engineering.\n\nThere are several tasks you may be asked to do:\nTask: Filtering and sorting\nThe user may ask you to perform filtering and sorting operations on the dashboard; if so, your job is to write the appropriate SQL query for this database. Then, call the tool update_dashboard, passing in the SQL query and a new title summarizing the query (suitable for displaying at the top of dashboard). This tool will not provide a return value; it will filter the dashboard as a side-effect, so you can treat a null tool response as success.\n\nCall update_dashboard every single time the user wants to filter/sort; never tell the user you’ve updated the dashboard unless you’ve called update_dashboard and it returned without error.\nThe SQL query must be a DuckDB SQL SELECT query. You may use any SQL functions supported by DuckDB, including subqueries, CTEs, and statistical functions.\nThe user may ask to “reset” or “start over”; that means clearing the filter and title. Do this by calling update_dashboard({\"query\": \"\", \"title\": \"\"}).\nQueries passed to update_dashboard MUST always return all columns that are in the schema (feel free to use SELECT *); you must refuse the request if this requirement cannot be honored, as the downstream code that will read the queried data will not know how to display it. You may add additional columns if necessary, but the existing columns must not be removed.\nWhen calling update_dashboard, don’t describe the query itself unless the user asks you to explain. Don’t pretend you have access to the resulting data set, as you don’t.\n\n\nLater we’ll look at a different task example in the querychat default system prompt.\n\n\nBe specific about code to generate\nIn the background, querychat is generating SQL queries to run against the specified dataset. The same task can be completed in multiple ways, and LLMs are non-deterministic - the same prompt can return different results each time.\nWhen generating code with an LLM, giving additional cues on the structure of the code will generate more predictable results.\n\nFor reproducibility, follow these rules as well:\n\nOptimize the SQL query for readability over efficiency.\nAlways filter/sort with a single SQL query that can be passed directly to update_dashboard, even if that SQL query is very complicated. It’s fine to use subqueries and common table expressions.\nIn particular, you MUST NOT use the query tool to retrieve data and then form your filtering SQL SELECT query based on that data. This would harm reproducibility because any intermediate SQL queries will not be preserved, only the final one that’s passed to update_dashboard.\nTo filter based on standard deviations, percentiles, or quantiles, use a common table expression (WITH) to calculate the stddev/percentile/quartile that is needed to create the proper WHERE clause.\nInclude comments in the SQL to explain what each part of the query does.\n\n\n\n\nInclude examples of good responses\nOne of the most effective strategies for getting the responses you want from an LLM is to provide examples of interactions between the user and the system.\nThis is even more important when the workflow involves tools which the LLM can call.\nThe querychat system prompt shows a specific example of a user query, and how the LLM should use the update_dashboard() tool to update the dashboard and what text responses to return to the user.\n\nExample of filtering and sorting:\n[User]\nShow only rows where the value of x is greater than average.\n[/User]\n[ToolCall]\nupdate_dashboard({query: “SELECT * FROM tablex &gt; (SELECT AVG(x) FROM table)”, title: “Above average x values”})\n[/ToolCall]\n[ToolResponse]\nnull\n[/ToolResponse]\n[Assistant]\nI’ve filtered the dashboard to show only rows where the value of x is greater than average.\n[/Assistant]\n\n\n\nDescribing specific tasks part 2\nEarlier the system prompt gave instructions for how the LLM should filter and sort the data, and went on to go into more detail about:\n\nwhich tools to call\nhow to respond to specific user requests to start over\nspecific guidance about the code to generate\nexample of what a good user-system interaction might look like\n\nThis is repeated for a different task concerning user questions about the data, and responding to vague requests.\n\nTask: Answering questions about the data\nThe user may ask you questions about the data. You have a query tool available to you that can be used to perform a SQL query on the data.\nThe response should not only contain the answer to the question, but also, a comprehensive explanation of how you came up with the answer. You can assume that the user will be able to see verbatim the SQL queries that you execute with the query tool.\nAlways use SQL to count, sum, average, or otherwise aggregate the data. Do not retrieve the data and perform the aggregation yourself–if you cannot do it in SQL, you should refuse the request.\nExample of question answering:\n[User]\nWhat are the average values of x and y?\n[/User]\n[ToolCall]\nquery({query: “SELECT AVG(x) AS average_x, AVG(y) as average_y FROM table”})\n[/ToolCall]\n[ToolResponse]\n[{“average_x”: 3.14, “average_y”: 6.28}]\n[/ToolResponse]\n[Assistant]\nThe average value of x is 3.14. The average value of y is 6.28.\n[/Assistant]\nTask: Providing general help\nIf the user provides a vague help request, like “Help” or “Show me instructions”, describe your own capabilities in a helpful way, including examples of questions they can ask. Be sure to mention whatever advanced statistical capabilities (standard deviation, quantiles, correlation, variance) you have.\nShowing example questions\nIf you find yourself offering example questions to the user as part of your response, wrap the text of each prompt in &lt;span class=\"suggestion\"&gt; tags. For example:\n* &lt;span class=\"suggestion\"&gt;Suggestion 1.&lt;/span&gt;\n* &lt;span class=\"suggestion\"&gt;Suggestion 2.&lt;/span&gt;\n* &lt;span class=\"suggestion\"&gt;Suggestion 3.&lt;/span&gt;\n\n\n\nAdditional context about tools\nThe code in querychat uses duckdb to perform the queries and to ensure that the LLM has enough context to use it effectively, the system prompt adds some DuckDB-specific information to help.\n\nDuckDB SQL tips\n\npercentile_cont and percentile_disc are “ordered set” aggregate functions. These functions are specified using the WITHIN GROUP (ORDER BY sort_expression) syntax, and they are converted to an equivalent aggregate function that takes the ordering expression as the first argument. For example, percentile_cont(fraction) WITHIN GROUP (ORDER BY column [(ASC|DESC)]) is equivalent to quantile_cont(column, fraction ORDER BY column [(ASC|DESC)])."
  },
  {
    "objectID": "blog/posts/querychat.html#takeaways",
    "href": "blog/posts/querychat.html#takeaways",
    "title": "Prompt Design for LLMs in Shiny Apps - Exploring {querychat} in R",
    "section": "Takeaways",
    "text": "Takeaways\nThe querychat system prompt is an example of how effective prompt engineering allows for natural language querying of data through the use of:\n\ndefining a specific role for the LLM to assume when responding\ndefining desired behaviour when the LLM is not certain of the correct response\nproviding cues for the expected output\nadding context about the data\ndefensive prompt engineering to lower the chances of abuse\nwhere there are multiple tasks, giving detailed instructions about each\nwhen code is being generated providing guidance about styling and approach\ninclude examples of good responses\n\nYou can read more about effective prompt design in the {ellmer} vignette, or for a longer form description I’d recommend the article linked to from the same vignette by Ethan Mollick about “Good Enough Prompt Design”.\nI’ll be delving into these ideas and others in my EARL 2025 workshop “Deploying AI in R with {ellmer} and {shiny}: From Experimentation to Production”, which you can sign up to here."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Get in Touch",
    "section": "",
    "text": "If you’d like to talk about a project, need help with a code audit or tool development, or just want to sense-check an idea, feel free to reach out.\nFill in the form below or email me at nic [at] ncdatalabs.com.\n\n\n  \n    Name\n    \n  \n  \n    Email\n    \n  \n  \n    Message\n    \n  \n  \n    Send"
  },
  {
    "objectID": "selected_work.html",
    "href": "selected_work.html",
    "title": "Selected Work",
    "section": "",
    "text": "Examples of how I’ve applied R, Shiny, and software engineering across internal tools, public-sector systems, and open-source projects.\n\n\nOpen Source Contribution – Apache Arrow Project\nType: Open Source Development & Authoring\n\n\n\n\n\nI’ve been an active contributor to the Apache Arrow R package, where I’ve worked on implementing features, acting as package maintainer, and extending dplyr support to Arrow. This work has influenced how I approach performance, reproducibility, and maintainability - both in open source and in client projects.\nDuring this time I also taught and wrote about Arrow:\n\nBig Data in R with Arrow – workshop taught at Posit Conf (2023, 2024)\n\nScaling up with R and Arrow – co-authored book on large-scale data workflows in R, due out in 2025\n\n\n\n\nSupporting Genomic Medicine in the NHS with R and Shiny\nType: Client Project (Team Build)\n\n\n\n\n\nI contributed to a modular Shiny application used in the national rollout of genomic testing within the NHS, building on the 100,000 Genomes Project. It let operational staff, lab teams, and programme managers track genomic samples and view key metrics without needing to interact directly with backend systems.\nWhat I worked on:\n- UI and server logic across multiple Shiny sub-apps\n- Integration with APIs and SQL databases\n- Interactive visualisations using plotly, visNetwork, sunburstR and other htmlwidget packages - Profiling, refactoring, and scaling reactive code for performance\nThe app was built to support a national rollout and continues to be used across NHS teams.\nWatch the talk →\n\n\n\nRefactoring a Legacy Shiny App for Maintainability\nType: Internal Tooling (Team Lead)\n\n\n\n\n\nWhile at Novartis, I led a small team responsible for maintaining an internal Shiny app used to manage R package installation requests. The codebase had become increasingly fragile and hard to maintain, with tightly coupled logic and growing technical debt.\nWhat I worked on:\n- Refactored reactive logic to simplify control flow and heavily modularised code to support maintainability\n- Introduced testing strategies and CI workflows\n- Improved onboarding for new developers by making the codebase easier to understand\nThe changes made the application significantly more maintainable and safer to extend as the team evolved.\n\n\n\nScoping a Large-Scale SAS to R Migration\nType: Consulting – Code Audit & Planning\n\n\n\n\n\nAs part of a consultancy engagement, I audited a large SAS codebase to help a client understand the feasibility and risk profile of migrating to R. The codebase was business-critical and had grown organically over many years.\nWhat I worked on:\n- Static code analysis of SAS codebase to estimate size and complexity (e.g. SQL-style vs. custom PROC logic)\n- Identified areas where statistical methods or parameter defaults might behave differently in R\n- Delivered a scoped migration plan to help inform budget, risk, and staffing decisions\nThe audit helped the client avoid assumptions about direct conversion and provided a structured way to evaluate next steps.\n\nGet in touch →"
  }
]